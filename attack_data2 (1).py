# -*- coding: utf-8 -*-
"""attack_data2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQgl1vejz57y4Fclcjm1y0bkG1lMSOau
"""

pip install git+https://github.com/fra31/auto-attack

!pip install datasets
!pip install numpy
!pip install torch

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import torch
import random
from datasets import load_dataset
from PIL import Image
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
from torchvision import transforms
import torch
from torch.utils.data import DataLoader, TensorDataset

!pip install datasets
!pip install numpy
!pip install torch
# %pip install recoloradv
# mister_ed
import recoloradv.mister_ed.loss_functions as lf
import recoloradv.mister_ed.utils.pytorch_utils as utils
import recoloradv.mister_ed.utils.image_utils as img_utils
import recoloradv.mister_ed.cifar10.cifar_loader as cifar_loader
import recoloradv.mister_ed.cifar10.cifar_resnets as cifar_resnets
import recoloradv.mister_ed.adversarial_training as advtrain
import recoloradv.mister_ed.utils.checkpoints as checkpoints
import recoloradv.mister_ed.adversarial_perturbations as ap
import recoloradv.mister_ed.adversarial_attacks as aa
import recoloradv.mister_ed.spatial_transformers as st
import recoloradv.mister_ed.config as config
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from autoattack import AutoAttack
from torchvision import models

from google.colab import drive
drive.mount('/content/drive')

val = load_dataset("Maysee/tiny-imagenet", split="valid") #  change to train dataset

class ImageNetDataset(Dataset):
    def __init__(self, huggingface_dataset, transform=None):
        """
        Args:
            huggingface_dataset: Our ImageNet dataset from huggingface
            transform: Potential transformation for the images
        """
        self.dataset = huggingface_dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image = self.dataset[idx]['image']
        label = self.dataset[idx]['label']

        # Apply the transform if specified
        if self.transform:
            image = self.transform(image)

        return image, label


# Example transformation function: this is for use with Vision Transformers
transform = transforms.Compose([
    transforms.Resize((244, 244)),
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x), # apparently some images are not RGB
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])



model_path = 'fine_tune_classifier.pth'
model = models.efficientnet_b0(weights="IMAGENET1K_V1")
num_ftrs = model.classifier[1].in_features
model.classifier[1] = torch.nn.Linear(num_ftrs, 200)
state_dict = torch.load('/content/drive/MyDrive/Colab Notebooks/fine_tune_classifier.pth', weights_only=True, map_location=torch.device('cpu'))
model.load_state_dict(state_dict)
epsilon = 0.03
adversary = AutoAttack(model.forward, norm='Linf', eps=epsilon, version='rand')
model = model.to('cuda')
model.eval()
normalizer = utils.DifferentiableNormalize(mean=[0.485, 0.456, 0.406],
                                           std=[0.229, 0.224, 0.225])

dataset = ImageNetDataset(val, transform=transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
    #transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0) == 1 else x)
]))
val_loader = DataLoader( dataset,batch_size=30,shuffle=False)
adv_inputs_list = []
labels_list = []
batches_correct = []
batches_correct_org = []
for batch_index, (inputs, labels) in enumerate(val_loader):
    print(batch_index)
    if torch.cuda.is_available():
        inputs = inputs.cuda()
        labels = labels.cuda()
    adv_inputs = adversary.run_standard_evaluation(inputs, labels, bs=30)
    adv_inputs_list.append(adv_inputs)
    labels_list.append(labels)
    with torch.no_grad():
        adv_logits = model(adv_inputs)
        logits = model(inputs)
    batch_correct = (adv_logits.argmax(1) == labels).detach()
    batch_correct_org = (logits.argmax(1) == labels).detach()
    batch_accuracy = batch_correct.float().mean().item()
    batch_accuracy_org = batch_correct_org.float().mean().item()
    batches_correct.append(batch_correct)
    batches_correct_org.append(batch_correct_org)
 #   if(batch_index==0):
 #     break
accuracy = torch.cat(batches_correct).float().mean().item()
accuracy_org = torch.cat(batches_correct_org).float().mean().item()
print('OVERALL ', f'accuracy = {accuracy * 100:.1f}',sep='\t')
print('OVERALL ',f'accuracy = {accuracy_org * 100:.1f}',sep='\t')

adv_inputs_all = torch.cat(adv_inputs_list, dim=0)
labels_all = torch.cat(labels_list, dim=0)

from torchvision import transforms

unnormalize = transforms.Normalize(mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.225],std=[1 / 0.229, 1 / 0.224, 1 / 0.225])

import matplotlib.pyplot as plt
import numpy as np
def plot(input, adv_inputs, labels, adv_labels, num_images=2):
   images = input.detach().cpu()
    adv_images = adv_inputs.detach().cpu()
   images = torch.stack([unnormalize(img).clamp(0, 1) for img in images])
    adv_images = torch.stack([unnormalize(img).clamp(0, 1) for img in adv_images])
   images =images.permute(0, 2, 3, 1).numpy()
    adv_images = adv_images.permute(0, 2, 3, 1).numpy()
    fig, axes = plt.subplots(num_images, 2, figsize=(10, num_images * 5))
    for i in range(num_images):
        axes[i, 0].imshow(images[i])
        axes[i, 0].set_title(f"Label: {labels[i].item()}")
        axes[i, 0].axis('off')
        axes[i, 1].imshow(adv_images[i])
        axes[i, 1].set_title(f"Predicted: {adv_labels[i].item()}")
        axes[i, 1].axis('off')
    plt.show()
with torch.no_grad():
    adv_logits = model(adv_inputs)
adv_preds = adv_logits.argmax(1)

plot(inputs, adv_inputs, labels, adv_preds, num_images=5)

val_dataset = TensorDataset(adv_inputs_all,labels_all)
torch.save(val_dataset, '/content/drive/MyDrive/Colab Notebooks/val_dataset2.pt')

